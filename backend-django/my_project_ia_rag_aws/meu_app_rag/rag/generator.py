import boto3
from typing import List, Dict, Optional
from decimal import Decimal
from langchain_aws import ChatBedrock
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from config.settings_rag import (
    AWS_REGION, 
    BEDROCK_MODEL_ID,
    MAX_TOKENS,
    TEMPERATURE,
    TOP_P,
    HISTORICO_MAX
)


class ResponseGenerator:
    """
    RAG - Generation: Gerador de respostas usando Claude via AWS Bedrock.
    
    Funcionalidades:
    - Gera√ß√£o de respostas contextualizadas
    - Hist√≥rico de conversa√ß√£o
    - Diferentes modos de resposta (r√°pida, detalhada)
    - Formata√ß√£o inteligente de produtos
    - Tratamento de edge cases
    """

    def __init__(self):
        """Inicializa o gerador com cliente AWS Bedrock"""
        self.client = boto3.client(
            service_name="bedrock-runtime",
            region_name=AWS_REGION
        )

        self.model = ChatBedrock(
            model_id=BEDROCK_MODEL_ID,
            client=self.client,
            model_kwargs={
                "max_tokens": MAX_TOKENS,
                "temperature": TEMPERATURE,
                "top_p": TOP_P
            }
        )

        # Hist√≥rico de conversa√ß√£o
        self.historico: List[Dict[str, str]] = []
        
        print(f"‚úÖ Generator inicializado: {BEDROCK_MODEL_ID}")

    def _contexto_invalido(self, contexto: str) -> bool:
        """
        Verifica se o contexto est√° vazio ou inv√°lido.
        
        Args:
            contexto: Contexto gerado pelo augmenter
            
        Returns:
            bool: True se o contexto √© inv√°lido
        """
        if not contexto:
            return True

        ctx = contexto.strip().lower()

        # Casos claros de contexto vazio
        if (
            ctx == "" 
            or ctx.startswith("nenhum produto") 
            or ctx.startswith("nenhuma informa√ß√£o")
            or len(ctx) < 20
        ):
            return True

        # Se n√£o cont√©m pelo menos 1 produto formatado
        if "id:" not in ctx and "nome:" not in ctx and "produto" not in ctx:
            return True

        return False

    def _formatar_preco(self, preco: float) -> str:
        """
        Formata pre√ßo para exibi√ß√£o em Real brasileiro.
        
        Args:
            preco: Valor num√©rico do pre√ßo
            
        Returns:
            str: Pre√ßo formatado (ex: R$ 99,90)
        """
        if isinstance(preco, (Decimal, float, int)):
            return f"R$ {float(preco):,.2f}".replace(",", "X").replace(".", ",").replace("X", ".")
        return str(preco)

    def _criar_system_prompt(self, modo: str = "normal") -> str:
        """
        Cria prompt do sistema baseado no modo.
        
        Args:
            modo: Tipo de resposta ('normal', 'rapida', 'detalhada')
            
        Returns:
            str: System prompt formatado
        """
        base_prompt = """
Voc√™ √© um assistente de compras virtual especializado, amig√°vel e prestativo.

üéØ SUA MISS√ÉO:
Ajudar clientes a encontrar produtos ideais usando APENAS o cat√°logo fornecido.

üìã REGRAS FUNDAMENTAIS:
1. ‚úÖ USE APENAS informa√ß√µes dos produtos no contexto fornecido
2. ‚úÖ Seja claro, objetivo e amig√°vel (tom conversacional)
3. ‚úÖ Mencione pre√ßos, marcas e caracter√≠sticas relevantes
4. ‚úÖ Destaque promo√ß√µes quando houver pre√ßo promocional
5. ‚úÖ Destaque avalia√ß√µes altas (‚â•4.5 estrelas)
6. ‚úÖ Avise se estoque est√° baixo (<10 unidades)
7. ‚úÖ Compare produtos quando houver m√∫ltiplas op√ß√µes
8. ‚ùå NUNCA invente produtos, pre√ßos ou informa√ß√µes
9. ‚ùå NUNCA use conhecimento externo ao cat√°logo
10. ‚ùå Se n√£o encontrar, seja honesto: "N√£o encontrei esse produto."

üí° FORMATO DE RESPOSTA:
"""

        if modo == "rapida":
            base_prompt += """
- Seja MUITO CONCISO (2-3 frases no m√°ximo)
- Liste apenas nome, pre√ßo e 1 caracter√≠stica chave
- Use bullet points para m√∫ltiplos produtos
"""
        elif modo == "detalhada":
            base_prompt += """
- Seja COMPLETO e DESCRITIVO
- Inclua: pre√ßo, marca, caracter√≠sticas, estoque, avalia√ß√µes
- Compare vantagens entre produtos
- Sugira alternativas quando apropriado
- Use emojis moderadamente para destacar pontos importantes
"""
        else:  # normal
            base_prompt += """
- Seja EQUILIBRADO entre conciso e informativo
- Mencione: pre√ßo, marca e 2-3 caracter√≠sticas principais
- Destaque o melhor custo-benef√≠cio
- Use tom amig√°vel e profissional
"""

        base_prompt += """

üìù EXEMPLOS DE BOAS RESPOSTAS:

‚ùì "Quero uma camiseta branca"
‚úÖ "Encontrei a Camiseta B√°sica Branca da BasicWear por R$ 39,90. √â 100% algod√£o, 
    tem √≥tima avalia√ß√£o (4.3‚≠ê) e est√° dispon√≠vel no tamanho M com 150 unidades 
    em estoque."

‚ùì "T√™nis at√© 200 reais"
‚úÖ "Tenho o T√™nis Corrida Pro Run em PROMO√á√ÉO! De R$ 199,90 por R$ 149,90 üî•
    √â da SportPro, tem solado EVA e avalia√ß√£o excelente (4.8‚≠ê). 
    30 unidades dispon√≠veis no tamanho 42."

‚ùì "Perfume importado"
‚úÖ "O Perfume Masculino Intense da FragrancePro √© perfeito! R$ 289,90 pelos 100ml.
    Tem notas amadeiradas e c√≠tricas, concentra√ß√£o de 15% e avalia√ß√£o 4.9‚≠ê. 
    √â sofisticado e de longa dura√ß√£o. Estoque baixo: apenas 8 unidades!"

üö´ NUNCA FA√áA:
‚ùå "Temos tamb√©m o T√™nis Nike Air Max..." (produto n√£o est√° no cat√°logo)
‚ùå "Este produto √© o melhor do mercado..." (opini√£o n√£o baseada em dados)
‚ùå "Entrega em 2 dias..." (info n√£o fornecida no cat√°logo)
"""

        return base_prompt

    def generate(
        self, 
        query: str, 
        context: str,
        modo: str = "normal",
        incluir_historico: bool = False
    ) -> str:
        """
        Gera resposta baseada na consulta e contexto.
        
        Args:
            query: Pergunta do usu√°rio
            context: Contexto dos produtos encontrados (do augmenter)
            modo: Tipo de resposta ('normal', 'rapida', 'detalhada')
            incluir_historico: Se deve incluir hist√≥rico da conversa
            
        Returns:
            str: Resposta gerada pelo LLM
        """
        # Validar modo
        if modo not in ['normal', 'rapida', 'detalhada']:
            modo = 'normal'

        # Se contexto n√£o tem produto ‚Üí retorno autom√°tico
        if self._contexto_invalido(context):
            return (
                "üòî N√£o encontrei produtos que correspondam √† sua busca.\n\n"
                "üí° Dicas:\n"
                "‚Ä¢ Tente usar termos mais gerais (ex: 't√™nis' em vez de 't√™nis Nike Air')\n"
                "‚Ä¢ Verifique a ortografia\n"
                "‚Ä¢ Busque por categoria (Roupas, Eletr√¥nicos, Beleza, etc.)\n"
                "‚Ä¢ Pergunte sobre faixas de pre√ßo"
            )

        # Criar prompt do sistema
        system_prompt = self._criar_system_prompt(modo)
        
        # Adicionar cat√°logo ao prompt
        system_prompt += f"\n\nüì¶ CAT√ÅLOGO DISPON√çVEL:\n\n{context}"

        # Construir mensagens
        messages = [SystemMessage(content=system_prompt)]

        # Adicionar hist√≥rico se solicitado
        if incluir_historico and self.historico:
            for msg in self.historico[-HISTORICO_MAX * 2:]:
                if msg['role'] == 'user':
                    messages.append(HumanMessage(content=msg['content']))
                elif msg['role'] == 'assistant':
                    messages.append(AIMessage(content=msg['content']))

        # Adicionar consulta atual
        messages.append(HumanMessage(content=query))

        try:
            # Invocar modelo
            resposta = self.model.invoke(messages).content.strip()

            # P√≥s-processar resposta
            resposta = self._pos_processar_resposta(resposta)

            # Salvar no hist√≥rico
            self.historico.append({
                "role": "user",
                "content": query
            })
            self.historico.append({
                "role": "assistant",
                "content": resposta
            })

            # Manter apenas √∫ltimas N intera√ß√µes
            if len(self.historico) > HISTORICO_MAX * 2:
                self.historico = self.historico[-HISTORICO_MAX * 2:]

            return resposta

        except Exception as e:
            error_msg = str(e)
            print(f"‚ùå Erro ao gerar resposta: {error_msg}")
            
            return (
                "‚ùå Desculpe, houve um erro ao processar sua solicita√ß√£o.\n\n"
                f"Detalhes t√©cnicos: {error_msg}\n\n"
                "Por favor, tente novamente em alguns instantes."
            )

    def _pos_processar_resposta(self, resposta: str) -> str:
        """
        P√≥s-processa a resposta para melhorar formata√ß√£o.
        
        Args:
            resposta: Resposta bruta do LLM
            
        Returns:
            str: Resposta formatada
        """
        # Remove espa√ßos extras
        resposta = resposta.strip()
        
        # Remove quebras de linha excessivas
        while "\n\n\n" in resposta:
            resposta = resposta.replace("\n\n\n", "\n\n")
        
        # Garante que n√£o termine com pontos m√∫ltiplos
        while resposta.endswith(".."):
            resposta = resposta[:-1]
        
        return resposta

    def generate_rapida(self, query: str, context: str) -> str:
        """
        Gera resposta r√°pida e concisa (modo express).
        
        Args:
            query: Pergunta do usu√°rio
            context: Contexto dos produtos
            
        Returns:
            str: Resposta concisa
        """
        return self.generate(query, context, modo='rapida')

    def generate_detalhada(self, query: str, context: str) -> str:
        """
        Gera resposta detalhada e completa.
        
        Args:
            query: Pergunta do usu√°rio
            context: Contexto dos produtos
            
        Returns:
            str: Resposta detalhada
        """
        return self.generate(query, context, modo='detalhada')

    def generate_com_historico(self, query: str, context: str) -> str:
        """
        Gera resposta considerando hist√≥rico de conversa√ß√£o.
        
        Args:
            query: Pergunta do usu√°rio
            context: Contexto dos produtos
            
        Returns:
            str: Resposta contextualizada com hist√≥rico
        """
        return self.generate(query, context, incluir_historico=True)

    def generate_comparacao(
        self, 
        query: str, 
        produtos: List[Dict]
    ) -> str:
        """
        Gera compara√ß√£o entre produtos espec√≠ficos.
        
        Args:
            query: Pergunta sobre compara√ß√£o
            produtos: Lista de produtos a comparar
            
        Returns:
            str: Compara√ß√£o detalhada
        """
        # Criar contexto de compara√ß√£o
        context_parts = []
        for i, p in enumerate(produtos, 1):
            context_parts.append(
                f"\n{'='*50}\n"
                f"PRODUTO {i}:\n"
                f"Nome: {p.get('nome', 'N/A')}\n"
                f"Pre√ßo: {self._formatar_preco(p.get('preco', 0))}\n"
                f"Marca: {p.get('marca', 'N/A')}\n"
                f"Avalia√ß√£o: {p.get('avaliacao', 0)}‚≠ê ({p.get('num_avaliacoes', 0)} avalia√ß√µes)\n"
                f"Estoque: {p.get('estoque', 0)} unidades\n"
                f"Descri√ß√£o: {p.get('descricao', 'N/A')}\n"
            )
            
            if p.get('preco_promocional'):
                context_parts.append(
                    f"üî• PROMO√á√ÉO: {self._formatar_preco(p['preco_promocional'])}\n"
                )
        
        context = "\n".join(context_parts)
        
        # Prompt espec√≠fico para compara√ß√£o
        query_comparacao = f"""
Compare os produtos listados focando em:
1. Melhor custo-benef√≠cio
2. Diferen√ßas de qualidade/avalia√ß√£o
3. Pre√ßo e promo√ß√µes
4. Disponibilidade (estoque)

Consulta original: {query}

Fa√ßa uma compara√ß√£o clara e objetiva ajudando na decis√£o de compra.
"""
        
        return self.generate_detalhada(query_comparacao, context)

    def clear_history(self):
        """Limpa o hist√≥rico de conversa√ß√£o"""
        self.historico = []
        print("‚úÖ Hist√≥rico limpo")

    def get_history(self) -> List[Dict[str, str]]:
        """
        Retorna o hist√≥rico de conversa√ß√£o.
        
        Returns:
            list: Lista de mensagens do hist√≥rico
        """
        return self.historico

    def get_history_formatted(self) -> str:
        """
        Retorna hist√≥rico formatado para visualiza√ß√£o.
        
        Returns:
            str: Hist√≥rico formatado
        """
        if not self.historico:
            return "Nenhum hist√≥rico dispon√≠vel."
        
        formatted = []
        for msg in self.historico:
            role = "üë§ Usu√°rio" if msg['role'] == 'user' else "ü§ñ Assistente"
            formatted.append(f"{role}: {msg['content']}\n")
        
        return "\n".join(formatted)

    def get_statistics(self) -> Dict:
        """
        Retorna estat√≠sticas do gerador.
        
        Returns:
            dict: Estat√≠sticas de uso
        """
        total_mensagens = len(self.historico)
        mensagens_usuario = sum(1 for m in self.historico if m['role'] == 'user')
        mensagens_assistente = sum(1 for m in self.historico if m['role'] == 'assistant')
        
        return {
            "total_mensagens": total_mensagens,
            "mensagens_usuario": mensagens_usuario,
            "mensagens_assistente": mensagens_assistente,
            "modelo": BEDROCK_MODEL_ID,
            "max_tokens": MAX_TOKENS,
            "temperature": TEMPERATURE,
            "historico_max": HISTORICO_MAX
        }