# ğŸ§  COMO Ã‰ REALIZADO O "INPUTAR CONHECIMENTO" - EXPLICAÃ‡ÃƒO COMPLETA

---

## ğŸ“Š FLUXO COMPLETO EM 3 ETAPAS

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ETAPA 1: ADICIONAR PRODUTOS NO BANCO DE DADOS               â”‚
â”‚ Arquivo: adicionar_produtos.py                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ETAPA 2: GERAR EMBEDDINGS                                   â”‚
â”‚ Comando: python manage.py popular_embeddings --force        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ETAPA 3: BUSCA VETORIAL NO RAG                              â”‚
â”‚ Arquivo: retriever.py                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ ETAPA 1: ADICIONAR NO BANCO (adicionar_produtos.py)

### O que acontece:

```python
# Script: adicionar_produtos.py

produto = Produto.objects.create(
    nome="Camiseta BÃ¡sica Branca",
    categoria="Roupas",
    preco=39.90,
    descricao="Camiseta bÃ¡sica de algodÃ£o",
    # ... outros campos
)
```

### SQL executado (por trÃ¡s do Django ORM):

```sql
INSERT INTO meu_app_rag_produto (
    nome, 
    categoria, 
    preco, 
    descricao,
    ...
) VALUES (
    'Camiseta BÃ¡sica Branca',
    'Roupas',
    39.90,
    'Camiseta bÃ¡sica de algodÃ£o',
    ...
);
```

### Resultado:

```
PostgreSQL/SQLite
â”œâ”€â”€ Tabela: meu_app_rag_produto
â”‚   â”œâ”€â”€ ID: 1 | Nome: Camiseta BÃ¡sica
â”‚   â”œâ”€â”€ ID: 2 | Nome: CalÃ§a Jeans
â”‚   â”œâ”€â”€ ID: 3 | Nome: TÃªnis Corrida
â”‚   â””â”€â”€ ...
```

**âš ï¸ IMPORTANTE:** Neste ponto, os dados estÃ£o APENAS no banco de dados relacional. O RAG ainda NÃƒO consegue buscar por eles!

---

## ğŸ§  ETAPA 2: GERAR EMBEDDINGS (popular_embeddings.py)

### Comando:
```bash
python manage.py popular_embeddings --force
```

### O que acontece INTERNAMENTE:

#### **Passo 2.1: LER DO BANCO**

```python
# popular_embeddings.py - exportar_catalogo()

produtos = Produto.objects.all()  # â† SQL: SELECT * FROM produtos

catalogo = {}
for p in produtos:
    catalogo[p.id] = {
        'id': p.id,
        'nome': p.nome,
        'descricao': p.descricao,
        'categoria': p.categoria,
        'preco': float(p.preco),
        # ... todos os campos
    }

# Salva em arquivo pickle
with open('db_data/catalogo.pkl', 'wb') as f:
    pickle.dump(catalogo, f)
```

**Resultado:** Arquivo `catalogo.pkl` com todos os produtos

```python
# ConteÃºdo do catalogo.pkl
{
    1: {
        'id': 1,
        'nome': 'Camiseta BÃ¡sica Branca',
        'categoria': 'Roupas',
        'preco': 39.90,
        'descricao': 'Camiseta bÃ¡sica de algodÃ£o...',
        # ... todos os campos
    },
    2: {
        'id': 2,
        'nome': 'CalÃ§a Jeans Skinny',
        # ...
    },
    # ... 43 produtos
}
```

---

#### **Passo 2.2: GERAR EMBEDDINGS (AQUI Ã‰ A MÃGICA!)**

```python
# popular_embeddings.py - gerar_embeddings()

emb = Embeddings()  # Conecta com AWS Bedrock

ids = []
vectors = []

for pid, produto in catalogo.items():
    # 1. Criar texto descritivo
    texto = f"{produto['nome']}. {produto['descricao']}. Categoria: {produto['categoria']}"
    
    # Exemplo:
    # "Camiseta BÃ¡sica Branca. Camiseta bÃ¡sica de algodÃ£o. Categoria: Roupas"
    
    # 2. Normalizar (lowercase, sem acentos)
    texto_norm = unidecode(texto.lower())
    # "camiseta basica branca. camiseta basica de algodao. categoria: roupas"
    
    # 3. ENVIAR PARA AWS BEDROCK TITAN EMBEDDINGS
    vetor = emb.embed(texto_norm)
    
    # Retorna um vetor de 1024 nÃºmeros:
    # [0.234, -0.127, 0.891, 0.456, ..., -0.321]
    
    ids.append(pid)
    vectors.append(vetor)
```

### ğŸŒ O QUE ACONTECE NA AWS BEDROCK:

```
Texto: "camiseta basica branca..."
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS BEDROCK                    â”‚
â”‚   Amazon Titan Embeddings v2     â”‚
â”‚                                  â”‚
â”‚   Modelo de Machine Learning     â”‚
â”‚   treinado em bilhÃµes de textos  â”‚
â”‚                                  â”‚
â”‚   Converte texto em vetor que    â”‚
â”‚   representa o SIGNIFICADO       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
Vetor: [0.234, -0.127, 0.891, ..., -0.321]
       â†‘
       1024 dimensÃµes (nÃºmeros float)
```

**Por que 1024 nÃºmeros?**
- Cada nÃºmero representa uma "caracterÃ­stica semÃ¢ntica"
- Exemplo hipotÃ©tico:
  - DimensÃ£o 1: "Ã© roupa?" â†’ 0.9 (sim!)
  - DimensÃ£o 2: "Ã© eletrÃ´nico?" â†’ 0.1 (nÃ£o!)
  - DimensÃ£o 3: "Ã© casual?" â†’ 0.8 (sim!)
  - DimensÃ£o 4: "Ã© confortÃ¡vel?" â†’ 0.7 (sim!)
  - ... 1020 dimensÃµes a mais

---

#### **Passo 2.3: SALVAR VETORES**

```python
# Salvar em arquivo pickle
with open('db_data/vectors.pkl', 'wb') as f:
    pickle.dump({
        'ids': [1, 2, 3, 4, ...],
        'vectors': np.array([
            [0.234, -0.127, 0.891, ...],  # Vetor do produto 1
            [0.198, -0.115, 0.870, ...],  # Vetor do produto 2
            [0.050, 0.230, -0.450, ...],  # Vetor do produto 3
            # ... 43 vetores
        ])
    }, f)
```

**Resultado:** Arquivo `vectors.pkl`

```python
# Estrutura do vectors.pkl
{
    'ids': [1, 2, 3, 4, 5, ..., 43],
    'vectors': numpy.array([
        [0.234, -0.127, ...],  # 1024 nÃºmeros
        [0.198, -0.115, ...],  # 1024 nÃºmeros
        # ... 43 linhas (uma por produto)
    ])
}
```

---

## ğŸ” ETAPA 3: BUSCA VETORIAL (retriever.py)

### Quando usuÃ¡rio faz uma consulta:

```python
# Usuario pergunta:
query = "Quero uma camiseta confortÃ¡vel"
```

### O que acontece:

#### **Passo 3.1: GERAR EMBEDDING DA PERGUNTA**

```python
# retriever.py - retrieve()

# 1. Normaliza
query_norm = "quero uma camiseta confortavel"

# 2. Gera embedding da pergunta (AWS Bedrock)
query_vector = embedding.embed(query_norm)
# Retorna: [0.240, -0.130, 0.895, ..., -0.318]
```

---

#### **Passo 3.2: COMPARAR COM TODOS OS PRODUTOS**

```python
# 3. Carrega vetores dos produtos (do arquivo pickle)
product_vectors = [
    [0.234, -0.127, 0.891, ...],  # Produto 1: Camiseta BÃ¡sica
    [0.198, -0.115, 0.870, ...],  # Produto 2: CalÃ§a Jeans
    [0.050, 0.230, -0.450, ...],  # Produto 3: TÃªnis
    # ... 43 produtos
]

query_vector = [0.240, -0.130, 0.895, ...]  # Pergunta

# 4. Calcula similaridade (cosseno) entre query e CADA produto
scores = []

for product_vector in product_vectors:
    score = cosine_similarity(query_vector, product_vector)
    scores.append(score)

# Resultado:
# scores = [0.92, 0.15, 0.10, ...]
#          â†‘     â†‘     â†‘
#          Cam   CalÃ§a TÃªnis
```

### ğŸ“ MATEMÃTICA: SIMILARIDADE DO COSSENO

```python
def cosine_similarity(A, B):
    """
    Mede o Ã¢ngulo entre dois vetores
    Resultado: -1 a 1
    - 1.0 = vetores idÃªnticos (mesmo significado)
    - 0.0 = vetores perpendiculares (sem relaÃ§Ã£o)
    """
    dot_product = sum(a * b for a, b in zip(A, B))
    magnitude_A = sqrt(sum(a**2 for a in A))
    magnitude_B = sqrt(sum(b**2 for b in B))
    
    return dot_product / (magnitude_A * magnitude_B)
```

**VisualizaÃ§Ã£o geomÃ©trica (simplificada para 2D):**

```
         Camiseta (0.92) âœ…
              â€¢
             /
            / â† Ã¢ngulo pequeno
           /
    Consulta â€¢
          \
           \
            \ â† Ã¢ngulo grande
             \
              â€¢
            TÃªnis (0.10) âŒ
```

---

#### **Passo 3.3: ORDENAR E RETORNAR TOP-5**

```python
# 5. Ordena por score (maior primeiro)
resultados = sorted(zip(ids, scores), key=lambda x: x[1], reverse=True)

# 6. Pega top-5
top5 = resultados[:5]

# Resultado:
[
    (1, 0.92),  # ID 1: Camiseta BÃ¡sica - score alto!
    (9, 0.85),  # ID 9: Camiseta Estampada
    (14, 0.78), # ID 14: Camiseta Polo
    (2, 0.15),  # ID 2: CalÃ§a Jeans - score baixo
    (3, 0.10),  # ID 3: TÃªnis - score baixo
]

# 7. Carrega dados completos do catalogo.pkl
produtos_retornados = []
for product_id, score in top5:
    produto = catalogo[product_id]
    produto['score'] = score
    produtos_retornados.append(produto)
```

---

## ğŸ¯ RESUMO DO FLUXO COMPLETO

```
FASE 1: PREPARAÃ‡ÃƒO (vocÃª roda 1x)
===================================

adicionar_produtos.py
    â†“ (SQL INSERT)
PostgreSQL/SQLite
    â†“ (SQL SELECT)
popular_embeddings.py
    â†“ (HTTP para AWS)
AWS Bedrock Titan
    â†“ (Retorna vetores)
Salva em:
    â”œâ”€â”€ catalogo.pkl (dados dos produtos)
    â””â”€â”€ vectors.pkl  (embeddings)


FASE 2: CONSULTA (cada vez que usuÃ¡rio pergunta)
=================================================

UsuÃ¡rio: "camiseta confortÃ¡vel"
    â†“ (normaliza)
"camiseta confortavel"
    â†“ (HTTP para AWS)
AWS Bedrock Titan
    â†“ (Retorna vetor da query)
[0.240, -0.130, 0.895, ...]
    â†“ (carrega vectors.pkl)
Compara com 43 produtos
    â†“ (cosine similarity)
Scores: [0.92, 0.85, 0.78, 0.15, 0.10, ...]
    â†“ (ordena e pega top-5)
Top-5 produtos
    â†“ (carrega catalogo.pkl)
Produtos completos com scores
    â†“ (formata contexto)
Augmenter
    â†“ (envia para AWS)
Claude no Bedrock
    â†“
Resposta em linguagem natural
```

---

## ğŸ’¾ ARQUIVOS GERADOS

### `db_data/catalogo.pkl`
```python
{
    1: {
        'id': 1,
        'nome': 'Camiseta BÃ¡sica',
        'preco': 39.90,
        # ... todos os campos
    },
    # ... 43 produtos
}
```
**Tamanho:** ~100 KB  
**FunÃ§Ã£o:** Dados completos dos produtos

---

### `db_data/vectors.pkl`
```python
{
    'ids': [1, 2, 3, ..., 43],
    'vectors': array([
        [0.234, -0.127, 0.891, ...],  # 1024 nÃºmeros
        # ... 43 linhas
    ])
}
```
**Tamanho:** ~500 KB  
**FunÃ§Ã£o:** Embeddings para busca vetorial

---

## ğŸ”‘ CONCEITOS-CHAVE

### 1. **Embedding = RepresentaÃ§Ã£o MatemÃ¡tica do Significado**

```
Texto: "camiseta confortÃ¡vel"
    â†“ [Titan Embeddings]
Vetor: [0.24, -0.13, 0.89, ..., -0.32]
       â†‘
    Captura o "significado"
```

### 2. **Similaridade do Cosseno = Medida de Proximidade**

```
Quanto menor o Ã¢ngulo entre vetores,
mais similares sÃ£o os significados!

Camiseta & "camiseta confortÃ¡vel" â†’ Ã¢ngulo pequeno â†’ score alto (0.92)
TÃªnis & "camiseta confortÃ¡vel"    â†’ Ã¢ngulo grande  â†’ score baixo (0.10)
```

### 3. **Por que isso funciona?**

Porque o modelo Titan foi treinado em bilhÃµes de textos e aprendeu que:
- "camiseta" e "blusa" sÃ£o similares
- "confortÃ¡vel" e "macio" sÃ£o similares
- "camiseta" e "notebook" NÃƒO sÃ£o similares

---

## âš¡ POR QUE PRECISA REGENERAR EMBEDDINGS?

```
âŒ SE NÃƒO REGENERAR:

Adiciona produtos no banco
    â†“
vectors.pkl ainda tem apenas 5 produtos antigos
    â†“
RAG sÃ³ encontra os 5 antigos
    â†“
Novos produtos INVISÃVEIS para busca!


âœ… SE REGENERAR:

Adiciona produtos no banco (43 produtos)
    â†“
Roda popular_embeddings --force
    â†“
vectors.pkl agora tem 43 produtos
    â†“
RAG encontra TODOS os 43!
```

---

## ğŸ¬ RESUMÃƒO FINAL

**INPUTAR CONHECIMENTO = 2 PASSOS:**

1. **Adicionar dados no banco** (SQL INSERT)
2. **Gerar embeddings** (AWS Bedrock + salvar em pickle)

**RESULTADO:**
- Sistema consegue fazer busca SEMÃ‚NTICA
- Encontra produtos por significado, nÃ£o apenas por palavras exatas
- "camiseta confortÃ¡vel" encontra produtos mesmo que nÃ£o tenham exatamente essas palavras

**Entendeu como funciona?** ğŸš€